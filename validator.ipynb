{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "# import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "from BeamSearchTree import BeamSearchTreeNode\n",
    "import pyreader\n",
    "import numpy as np\n",
    "from attention import Batcher, construct_feed_dict, extract_results, get_evals,AttentionModel,get_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_path(tree, k=1):\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    def search_tree(path, node, path_prob):\n",
    "        if not node.children:\n",
    "            paths.append((path, path_prob))\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                search_tree(path + [child.token_id], child, path_prob + np.log(child.probability))\n",
    "\n",
    "    search_tree([], tree, 0)\n",
    "\n",
    "    sorted_paths = sorted(paths, key=lambda x: x[1], reverse=True)\n",
    "    return [path[0] for path in sorted_paths[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_max_rands = {\n",
    "    \"var\": 4750, \"function\": 2900, \"Class\": 440, \"attribute\": 2400, \"arg\": 2400\n",
    "}\n",
    "def map_token(map, token):\n",
    "    mask = 0\n",
    "    if token.startswith(\"(*) \"):\n",
    "        mask = 1\n",
    "        token = token.replace(\"(*) \", \"\")\n",
    "\n",
    "    if token in map:\n",
    "        return map[token], mask\n",
    "\n",
    "    # Not in map, is it an identifier?\n",
    "    if \"|\" in token:\n",
    "        spl = token.split(\"|\")\n",
    "        if spl[1] in map:\n",
    "            return map[spl[1]]\n",
    "        elif spl[0] in map:\n",
    "            return map[spl[0]]\n",
    "        return pyreader.oov_id\n",
    "\n",
    "    elif any([token.startswith(prefix) for prefix in [key for key in type_max_rands]]):\n",
    "        return pyreader.oov_id\n",
    "\n",
    "    raise KeyError(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 1\n",
    "batch_size = 1\n",
    "hidden_size = 50\n",
    "num_samples = 3\n",
    "lambda_type='state'\n",
    "max_attention=3\n",
    "model_path = './model/2017-07-03--00-10--421088'\n",
    "data_path='data_samples/mapping.map'\n",
    "with open(data_path, \"rb\") as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "vocab_size = len(word_to_id)\n",
    "inv_map = {v: k for k, v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_test_cases = [\"import\", \"os\", \"\\n\", \"class\", \"(*) Class291\", \":\", \"\\n\", \"§<indent>§\", \"def\", \"__init__\", \"(\", \"self\", \",\",\n",
    "#      \"(*) arg123\", \")\", \":\", \"\\n\", \"§<indent>§\", \"self\", \".\", \"(*) attribute|attribute172\", \"=\", \"arg123\", \"\\n\", \"\\n\",\n",
    "#      \"§<dedent>§\",\n",
    "#      \"def\", \"(*) function234\", \"(\", \"self\", \",\", \"(*) filename|arg432\", \")\", \":\", \"\\n\", \"§<indent>§\", \"with\", \"open\",\n",
    "#      \"(\", \"filename|arg432\", \",\", \"'r'\", \")\", \"as\", \"(*) f|var76\", \":\", \"\\n\", \"§<indent>§\", \"(*) lines|var91\", \"=\",\n",
    "#      \"f|var76\", \".\",\n",
    "#      \"readlines\", \"(\", \")\", \"\\n\", \"§<dedent>§\", \"return\", \"len\", \"(\", \"lines|var91\", \")\", \"\\n\", \"\\n\", \"§<dedent>§\",\n",
    "#      \"def\", \"(*) func|function921\", \"(\", \"self\", \",\", \"(*) arg191\", \")\", \":\", \"\\n\", \"§<indent>§\", \"(*) var543\", \"=\",\n",
    "#      \"os\", \".\", \"path\", \".\", \"join\", \"(\", \"self\", \".\"]\n",
    "all_test_cases = ['from', 'rest_framework', 'import', 'routers', '\\n', 'from', '__future__','import','unicode_literals',\n",
    "                 '\\n','from','django','import','forms','\\n','\\n','from','setuptools','import','find_packages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Attention Cell\n",
      "INFO:tensorflow:Restoring parameters from ./model/2017-07-03--00-10--421088\\model.tf\n",
      "----------------------------------------\n",
      "Token(input):  from\n",
      "Predicted:  ( ) <newline> self §PAD§\n",
      "Actual   :  rest_framework import routers <newline> from\n",
      "----------------------------------------\n",
      "Token(input):  rest_framework\n",
      "Predicted:  §OOV§ = §NUM§ , §OOV§\n",
      "Actual   :  import routers <newline> from __future__\n",
      "----------------------------------------\n",
      "Token(input):  import\n",
      "Predicted:  <newline> <newline> def §OOV§ §OOV§\n",
      "Actual   :  routers <newline> from __future__ import\n",
      "----------------------------------------\n",
      "Token(input):  routers\n",
      "Predicted:  <newline> <newline> §<dedent>§ def §OOV§\n",
      "Actual   :  <newline> from __future__ import unicode_literals\n",
      "----------------------------------------\n",
      "Token(input):  <newline>\n",
      "Predicted:  ( ) <newline> self §PAD§\n",
      "Actual   :  from __future__ import unicode_literals <newline>\n",
      "----------------------------------------\n",
      "Token(input):  from\n",
      "Predicted:  ( ) ) ) §PAD§\n",
      "Actual   :  __future__ import unicode_literals <newline> from\n",
      "----------------------------------------\n",
      "Token(input):  __future__\n",
      "Predicted:  <newline> §<dedent>§ def §OOV§ §OOV§\n",
      "Actual   :  import unicode_literals <newline> from django\n",
      "----------------------------------------\n",
      "Token(input):  import\n",
      "Predicted:  <newline> <newline> §<dedent>§ def §OOV§\n",
      "Actual   :  unicode_literals <newline> from django import\n",
      "----------------------------------------\n",
      "Token(input):  unicode_literals\n",
      "Predicted:  find_packages <newline> import django <newline>\n",
      "Actual   :  <newline> from django import forms\n",
      "----------------------------------------\n",
      "Token(input):  <newline>\n",
      "Predicted:  = self self self §PAD§\n",
      "Actual   :  from django import forms <newline>\n",
      "----------------------------------------\n",
      "Token(input):  from\n",
      "Predicted:  ( ) <newline> self .\n",
      "Actual   :  django import forms <newline> <newline>\n",
      "----------------------------------------\n",
      "Token(input):  django\n",
      "Predicted:  db import models <newline> from\n",
      "Actual   :  import forms <newline> <newline> from\n",
      "----------------------------------------\n",
      "Token(input):  import\n",
      "Predicted:  <newline> import django <newline> <newline>\n",
      "Actual   :  forms <newline> <newline> from setuptools\n",
      "----------------------------------------\n",
      "Token(input):  forms\n",
      "Predicted:  ] ] ] ] §PAD§\n",
      "Actual   :  <newline> <newline> from setuptools import\n",
      "----------------------------------------\n",
      "Token(input):  <newline>\n",
      "Predicted:  ( ) <newline> self §PAD§\n",
      "Actual   :  <newline> from setuptools import find_packages\n",
      "Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    masks_ = tf.placeholder(tf.bool, [seq_length, batch_size, 1], name=\"masks\")\n",
    "    input_data_ = tf.placeholder(tf.int32, [seq_length, batch_size], name=\"inputs\")\n",
    "    targets_ = tf.placeholder(tf.float32, [seq_length, batch_size], name=\"targets\")\n",
    "    \n",
    "    a = AttentionModel(input_data=input_data_,\n",
    "                                 targets=targets_,\n",
    "                                 masks=masks_,\n",
    "                                 is_training=False,\n",
    "                                 attention_num= 1,\n",
    "                                 batch_size=batch_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 num_samples=num_samples,\n",
    "                                 seq_length=seq_length,\n",
    "                                 vocab_size=vocab_size,\n",
    "                                 lambda_type=lambda_type,\n",
    "                                 max_attention=max_attention)\n",
    "    \n",
    "    \n",
    "    variables = tf.trainable_variables()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(variables)\n",
    "    saver.restore(session, os.path.join(model_path, \"model.tf\"))\n",
    "    \n",
    "    prediction_op = tf.nn.top_k(a.predict, 5)\n",
    "    \n",
    "    to_eval = [prediction_op[0], prediction_op[1]]\n",
    "    evaluation = get_evals(to_eval, a)\n",
    "    \n",
    "    \n",
    "    def run_network(token_id, state, att_states, att_ids, att_counts):\n",
    "        att_mask = np.ones([1, 1])\n",
    "        if isinstance(token_id, tuple):\n",
    "            data = (np.array([[token_id[0]]]), np.array([[1]]), np.array([att_mask]), np.array([1]), np.array([1]))\n",
    "        else:\n",
    "            data = (np.array([[token_id]]), np.array([[1]]), np.array([att_mask]), np.array([1]), np.array([1]))\n",
    "        feed_dict, _ = construct_feed_dict(a, data, state, att_states, att_ids, att_counts)\n",
    "\n",
    "        results = session.run(evaluation, feed_dict)\n",
    "        results, state, att_states, att_ids, alpha_states, att_counts, _ = extract_results(results, evaluation, 2,\n",
    "                                                                                           a)\n",
    "        return results, state, att_states, att_counts\n",
    "\n",
    "    def beam(tree_node):\n",
    "        # Populate the children of tree_node\n",
    "        init_state, init_att_states, init_att_counts = tree_node.state\n",
    "        results, state, att_states, att_counts = run_network(tree_node.token_id, init_state, init_att_states,\n",
    "                                                             att_ids, init_att_counts)\n",
    "        probs = results[0]\n",
    "        predict_ids = results[1]\n",
    "        for i in range(1,5):\n",
    "            tree_node.add_child(BeamSearchTreeNode(predict_ids[0, i],np.ones([1, 1]),\n",
    "                                                   (state, att_states, att_counts), probs[0, i]))\n",
    "\n",
    "    def beam_search_recursive(tree, current_depth):\n",
    "        if current_depth < 5:\n",
    "            for child in tree.children:\n",
    "                beam(child)\n",
    "                beam_search_recursive(child, current_depth + 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    state, att_states, att_ids, att_counts = get_initial_state(a, session)\n",
    "    count=0\n",
    "    accuracy=0\n",
    "    for i, token in enumerate(all_test_cases[:-5]):\n",
    "        lst =np.ones([1, 1])\n",
    "        token_id = map_token(word_to_id, token)\n",
    "        \n",
    "        if isinstance(token_id, tuple):\n",
    "            data = (np.array([[token_id[0]]]), np.array([[1]]), np.array([lst]), np.array([1]), np.array([1]))\n",
    "        else:\n",
    "            data = (np.array([[token_id]]), np.array([[1]]), np.array([lst]), np.array([1]), np.array([1]))\n",
    "        results, state, att_states, att_counts = run_network(map_token(word_to_id, token), state, att_states,\n",
    "                                                                     att_ids, att_counts)\n",
    "        root = BeamSearchTreeNode(map_token(word_to_id, all_test_cases[-1]),np.ones([1, 1]),\n",
    "                                          (state, att_states, att_counts), 1)\n",
    "        beam(root)\n",
    "        beam_search_recursive(root, 1)\n",
    "        path = find_path(root)[0]  # The most likely path\n",
    "        actual = [map_token(word_to_id, t) for t in all_test_cases[i + 1:i + 5 + 1]]\n",
    "\n",
    "        pred = \" \".join([inv_map[t].replace(\"\\n\", \"<newline>\") for t in path])\n",
    "        \n",
    "        act = []\n",
    "        for t in actual:\n",
    "            try:\n",
    "                act.append(inv_map[t[0]].replace(\"\\n\", \"<newline>\"))\n",
    "            except:\n",
    "                act.append(inv_map[t].replace(\"\\n\", \"<newline>\"))\n",
    "        act = ' '.join (act)\n",
    "        \n",
    "        print('-'*40)\n",
    "        print(\"Token(input): \", token.replace(\"\\n\", \"<newline>\"))\n",
    "        print(\"Predicted: \", pred)\n",
    "        print('Actual   : ', act)\n",
    "        count+=count+1\n",
    "        if pred == act:\n",
    "            accurate += 1\n",
    "    print(\"Accuracy: %f\" % (accuracy / count))\n",
    "#         results = session.run(evaluation, feed_dict)\n",
    "#         results_match = ''.join(str(results[1][0][0]))\n",
    "#         actual = [map_token(word_to_id, t) for t in all_test_cases[i + 1:i + 5 + 1]]\n",
    "#         act = \" \".join([inv_map[t[0]].replace(\"\\n\", \"<newline>\") for t in actual])\n",
    "#         print('=============')\n",
    "#         print('predict', inv_map[results[1][0][1]])\n",
    "#         print('real', act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 어큐러시가 낮은이유 : \n",
    "### 적은데이터셋트로 훈련을했었고, 훈련이 더필요함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
